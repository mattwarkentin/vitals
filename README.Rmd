---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# rinspect

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/rinspect)](https://CRAN.R-project.org/package=rinspect)
<!-- badges: end -->

rinspect is a framework for large language model evaluation in R. It's specifically aimed at ellmer users who want to measure the effectiveness of their LLM-based apps.

The package is an R port of the widely adopted Python framework [Inspect](https://inspect.ai-safety-institute.org.uk/). While the package doesn't integrate with Inspect directly, it allows users to interface with the [Inspect Log Viewer](https://inspect.ai-safety-institute.org.uk/log-viewer.html) and provides an on-ramp to transition to Inspect if need be by writing evaluation logs to the same file format.

> **Important**
> 
> ðŸš§ Under construction! ðŸš§
>
> rinspect is highly experimental and much of its documentation is aspirational.

## Installation

You can install the developmental version of rinspect using:

```r
pak::pak("simonpcouch/rinspect")
```

## Example

```{r}
#| echo: false
#| fig-alt: "A screencast of a Positron session. A script called sandbox.R is open in the editor with evaluation code for a 'simple addition' task. Executing `task_evaluate()` performs the evaluation, and the resulting eval is explored in the Inspect Log Viewer with `inspect_view()`."
knitr::include_graphics("https://github.com/simonpcouch/rinspect/blob/4949c1e00a1c429f147c3dbda6a656f7260620f4/inst/figs/rinspect.gif?raw=true")
```
