% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compare.R
\name{inspect_compare}
\alias{inspect_compare}
\title{Compare two task evaluations}
\usage{
inspect_compare(x, y, conf_level = 0.95)
}
\arguments{
\item{x, y}{\link{Task} objects arising from the same call to \code{Task$new()}.}

\item{conf_level}{Optional. A single numeric value between 0 and 1
representing the confidence level. Defaults to 0.95.}
}
\value{
A list containing statistics comparing the two task results
\itemize{
\item \code{mean_x}: The mean score of the first task evaluation (\code{x}).
\item \code{mean_y}: The mean score of the second task evaluation (\code{y}).
\item \code{mean_diff}: The mean difference between scores (\code{x} - \code{y}).
\item \code{paired_se}: The standard error of the paired differences.
\item \code{conf_int}: A vector containing the lower and upper bounds of the
confidence interval for the mean difference.
}

The function will error if the tasks have different
\code{task_id}s or different numbers of samples.
}
\description{
Compares performance metrics between two evaluations and calculates
statistical measures of their differences.
}
\details{
The statistics reported here are based on Section 4, \emph{Comparing models} in
"Adding Error Bars to Evals: A Statistical Approach to Language Model
Evaluations." Miller (2024), \url{https://arxiv.org/abs/2411.00640}.

Notably, confidence intervals are based on Equations (7) and (5) with
no clustering variable, and Equation (8) with clustering variables.
Epochs are treated as clustering variables, i.e. multiple samples based on
a given question are considered clustered.
}
\examples{
if (!identical(Sys.getenv("ANTHROPIC_API_KEY"), "") &&
    !identical(Sys.getenv("OPENAI_API_KEY"), "")) {
  library(ellmer)
  library(tibble)

  simple_addition <- tibble(
    input = c("What's 2+2?", "What's 2+3?"),
    target = c("4", "5")
  )

  # notably, both of the evaluations passed to `inspect_compare()`
  # will be based on the same call to `Task$new()`
  tsk <- Task$new(
    dataset = simple_addition,
    solver = generate(),
    scorer = model_graded_qa()
  )

  tsk_claude <- tsk$clone()
  tsk_claude$eval(solver_chat = chat_claude())

  tsk_openai <- tsk$clone()
  tsk_openai$eval(solver_chat = chat_openai())

  # do note that this is a rather silly comparison :)
  inspect_compare(tsk_claude, tsk_openai)
}
}
