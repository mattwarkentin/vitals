% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/task.R
\name{task_new}
\alias{task_new}
\alias{task}
\alias{task_evaluate}
\title{Evaluation tasks}
\usage{
task_new(dataset, solver, scorer, ..., name = generate_id())

task_evaluate(task, ...)
}
\arguments{
\item{dataset}{A tibble with, minimally, columns \code{input} and \code{target}.}

\item{solver}{A function that takes an element of \code{dataset$input} as input
and returns a value approximating \code{dataset$target} or an \link[ellmer:Chat]{ellmer::Chat}
object.}

\item{scorer}{A function that evaluates how well the solver's return value
approximates the corresponding elements of \code{dataset$target}.}

\item{...}{Named task parameters. The resulting \code{task} will accept any of
these parameters allowing for easily running variants of the task without
changing its source code.}
}
\description{
Evaluation \code{tasks} provide a flexible data structure for evaluating LLM-based
tools.
}
\examples{
\dontrun{

library(ellmer)
library(tibble)
library(glue)

dataset <- tibble(
  input = c("What's 2+2?", "What's 2+3?"),
  target = c("4", "5")
)

# requires an ANTHROPIC_API_KEY
solver <- function(input, ch = chat_claude()) {
  ch$clone()$chat(input)
}

# or, for more metadata...
solver <- chat_claude()

scorer <- function(input, target, output) {
  res <- chat_claude()$chat(glue::glue(
    "An assistant was asked the following: {input}\n",
    "The answer is: {target}.\n",
    "The assistant responded: {output}.\n",
    "Was the assistant correct?\n",
    "End your response with 'Answer: Yes' if yes, 'Answer: No' if no."
  ))

  if (grepl("Answer: Yes", res)) {
    return(TRUE)
  } else if (grepl("Answer: No", res)) {
    return(FALSE)
  } else {
    return(NA)
  }
}

task_new(
  dataset = dataset,
  solver = solver,
  scorer = scorer
)
}
}
