% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/task.R
\name{task_new}
\alias{task_new}
\alias{task}
\alias{task_evaluate}
\title{Evaluation tasks}
\usage{
task_new(name, dataset, solver, scorer, ...)

task_evaluate(task, ..., dir = eval_log_dir())
}
\arguments{
\item{dataset}{A tibble with, minimally, columns \code{input} and \code{target}.}

\item{solver}{A function that takes an element of \code{dataset$input} as input
and determines a value approximating \code{dataset$target}. Its return value should
be a list with elements \code{result} (the final response) and \code{chat} (an ellmer
chat used to solve the problem, or a list of them).

The rinspect package supplies a number of pre-built solvers; you might
start off with \code{\link[=generate]{generate()}}.}

\item{scorer}{A function that evaluates how well the solver's return value
approximates the corresponding elements of \code{dataset$target}.}

\item{...}{Named task parameters. The resulting \code{task} will accept any of
these parameters allowing for easily running variants of the task without
changing its source code.}
}
\description{
Evaluation \code{tasks} provide a flexible data structure for evaluating LLM-based
tools.
}
\examples{
if (interactive()) {

library(ellmer)
library(tibble)
library(glue)

dataset <- tibble(
  input = c("What's 2+2?", "What's 2+3?"),
  target = c("4", "5")
)

tsk <- task_new(
  name = "simple_addition",
  dataset = dataset,
  solver = generate(),
  scorer = model_graded_qa())
)

task_evaluate(tsk)
}
}
